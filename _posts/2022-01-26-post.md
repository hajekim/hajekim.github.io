---
title: "Airflow CeleryExecutor 설치하기 @ Local"
date: 2022-01-26 18:00:00 +0900
categories: 데이터 엔지니어링
tags: [Airflow, 데이터 파이프라인]
---
> Airbnb에서 만든 Workflow 관리 오픈소스 소프트웨어입니다.
많은 기업에서 데이터 파이프라인 운용에 인기 있게 사용되고 있습니다.
DAGs(Directed Acyclic Graphs) 형태로 Workflow 수행합니다.

## 사전 준비사항
- MySQL 설치
- Redis 설치

# Install
```
# Airflow 설치
pip3 install apache-airflow

# MySQL 플러그인 설치
pip3 install 'apache-airflow[mysql]'

# Celery 설치
pip3 install celery
```
Airflow와 메타데이터를 저장할 MySQL 플러그인을 설치합니다.

CeleryExecutor 사용을 위해서 Celery를 설치합니다.
![](https://images.velog.io/images/haje/post/e8b0f052-e24b-4485-bd28-16a71d797100/image.png)
참고로 SQLite가 디폴트로 사용이 가능합니다. Executor는 Sequential Executor만 사용 가능합니다. 다른 Executor 사용을 위해서는 메타데이터를 저장할 별도의 데이터베이스 구성을 해야 합니다.


# MySQL 환경 준비
```
mysql -u root -p

-- 스키마 생성
create database airflow;

-- 유저 생성
create user 'airflow'@'%' identified by 'Airflow!1';

-- airflow 유저에 airflow 스키마 권한 생성
grant all privileges on airflow.* to 'airflow'@'%';
flush privileges;
```
Root 계정으로 로그인하여 Schema, User를 생성합니다.


# Airflow 설정
```
# Timezone 설정
default_timezone = Asia/Seoul

# 메타데이터 저장소 설정
sql_alchemy_conn = mysql://airflow:Airflow!1@localhost:3306/airflow

# Executor DB 설정
result_backend = db+mysql://airflow:Airflow!1@localhost:3306/airflow

# Redis 설정
broker_url = redis://127.0.0.1:6379/0

# DAG 리스트 갱신 설정 (테스트니까 10초로)
dag_dir_list_interval = 10

# Executor를 설정
executor = CeleryExecutor
```
airflow.cfg를 수정합니다.

# Airflow 명령어
```
# Airflow DB 초기화
airflow db init

# Airflow Admin 권한의 유저 생성
airflow users create --role Admin --username [USERNAME] --email admin --firstname [FIRST_NAME] --lastname [LAST_NAME] --password [PASSWORD]

# Airflow Webserver 기동
# -p : 포트 설정
# -D : 데몬 설정
airflow webserver -p 8080 -D

# Airflow Scheduler
# -D : 데몬 설정
airflow scheduler -D

# Celery Worker
# -q : Queue 이름
airflow celery worker -q [QUEUE_NAME]
```

Celery Worker는 airflow.cfg에 명시하여 자동으로 worker가 기동이 됩니다.
만약 별도의 Worker를 띄우고자 한다면 CLI로 운용할 수 있습니다.
![](https://images.velog.io/images/haje/post/4a4c2ab5-40d6-4a68-82d4-15364ca0f708/image.png)

![](https://images.velog.io/images/haje/post/8cf2cd7f-351a-4aa7-b4d2-ed38f2f05075/image.png)
![](https://images.velog.io/images/haje/post/f5092624-a7fc-4e0d-a260-8fbb9fd58ac7/image.png)


## [Celery](https://github.com/celery/celery)
![](https://images.velog.io/images/haje/post/40a40bbd-435d-44df-89de-6d34edbb4b1a/image.png)
pip install로 별도로 Celery를 설치하는 것을 보고 뭐지? 하셨을 수 있을 거 같아요.
Celery는 Python으로 작성된 Asynchronous task queue/Job Queue입니다.
Job을 Broker를 통해서 전달하면 하나 이상의 Worker에서 처리하는 구조입니다.

![](https://images.velog.io/images/haje/post/16297ecf-905d-4d02-be2a-6e21a11888bf/image.png)

Airflow Celery Executor 사용을 위해서 본 문서에서는 Broker로 Redis를 사용하였습니다.

# 참조문서
- [Airflow Celery Executor](https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html)
- [Celery를 이용한 긴 작업 처리](https://spoqa.github.io/2012/05/29/distribute-task-with-celery.html)